---
title: "\ {BIOSTAT 285 Spring 2020 Homework 1}"
author: "Nan Liu"
output:
  pdf_document: default
  html_document:
    df_print: paged
subtitle: \textbf{Due 11:00 PM 04/17/2020 (Submit to CCLE)}
header-includes: \usepackage{amsgen,amsmath,amstext,amsbsy,amsopn,amssymb,mathabx,amsthm,bm,bbm}
urlcolor: blue
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
if(!require(ISLR)) { install.packages("ISLR", repos = "http://cran.us.r-project.org"); library(ISLR) }
if(!require(leaps)) { install.packages("leaps", repos = "http://cran.us.r-project.org"); library(leaps) }
if(!require(glmnet)) { install.packages("glmnet", repos = "http://cran.us.r-project.org"); library(glmnet) }
if(!require(pls)) { install.packages("pls", repos = "http://cran.us.r-project.org"); library(pls) }
```

\theoremstyle{definition}
\newtheorem*{hint}{Hint}

\theoremstyle{remark}
\newtheorem*{rmk}{Remark}

*Remark.* For **Computational Part**, please complete your answer in the **RMarkdown** file and summit the generated PDF and RMD files. Related packages have been loaded in setup.

## Computational Part

1. (Model Selection, [ISL] 6.8, *25 pt*) In this exercise, we will generate simulated data, and will then use this data to perform model selection.

  (a) Use the `rnorm` function to generate a predictor $\bm{X}$ of length $n = 100$, as well as a noise vector $\bm{\epsilon}$ of length $n = 100$.
    
    
```{r}
library(dplyr)
set.seed(1)
x <- rnorm(100)
epsilon <- rnorm(100)
```
    
  (b) Generate a response vector $\bm{Y}$ of length $n = 100$ according to the model $$ Y = \beta_0 + \beta_1 X + \beta_2 X^2 + \beta_3 X^3 + \epsilon, $$ where $\beta_0 = 3$, $\beta_1 = 2$, $\beta_2 = -3$, $\beta_3 = 0.3$.
    
```{r}
y <- 3 + 2*x -3*x^2 + 0.3*x^3 + epsilon
```
    

  (c) Use the `regsubsets` function from `leaps` package to perform best subset selection in order to choose the best model from the set of predictors $(X, X^2, \cdots, X^{10})$. What are the best models obtained according to $C_p$, BIC, and adjusted $R^2$, respectively? Show some plots to provide evidence for your answer, and report the coefficients of the best model obtained.
    
```{r}
library(leaps)
leaps <- regsubsets(y~x+I(x^2)+I(x^3)+I(x^4)+I(x^5)+I(x^6)+
                      I(x^7)+I(x^8)+I(x^9)+I(x^10),
                    data=data.frame(x=x,y=y),nvmax=10)
(summary <- summary(leaps))
```



```{r}
#Cp
plot(1:10, summary$cp, xlab="Number of Parameters", ylab="Cp", type = "l")
min_cp <- min(summary$cp)
points(c(1:10)[summary$cp==min_cp], min_cp, pch=1, col="purple")
#report the coefficients
lm1<-lm(y~I(x)+I(x^2)+I(x^7), data=data.frame(x=x,y=y) )
summary(lm1)
```
According to Cp, the model contains 3 predictors which are $X, X^2, X^7$
The best models obtained according to $C_p$ is: $Y$ = 3.08 + 2.36 * $X$-3.17* $X^2$ + 0.01 * $X^7$

```{r}
#BIC
plot(1:10, summary$bic, xlab="Number of Parameters", ylab="BIC", type = "l")
min_bic <- min(summary$bic)
points(c(1:10)[summary$bic==min_bic], min_bic, pch=1, col="purple")

#report the coefficients
lm2<-lm(y~I(x)+I(x^2)+I(x^7), data=data.frame(x=x,y=y) )
summary(lm2)
```
According to BIC, the model contains 3 predictors which are $X, X^2, X^7$
The best models obtained according to BIC is: $Y$ = 3.08 + 2.36 * $X$-3.17* $X^2$ + 0.01 * $X^7$

```{r}
#Adjusted R^2
plot(1:10, summary$adjr2, xlab="Number of Parameters", ylab="Adjusted R^2", type = "l")
max_adjr2 <- max(summary$adjr2)
points(c(1:10)[summary$adjr2==max_adjr2], max_adjr2, pch=1, col="purple")

#report the coefficients
lm3<-lm(y~I(x)+I(x^2)+I(x^7), data=data.frame(x=x,y=y) )
summary(lm3)
```

According to adjusted $R^2$, the model contains 3 predictors which are $X, X^2, X^7$
The best models obtained according to adjusted $R^2$ is: $Y$ = 3.08 + 2.36 * $X$-3.17* $X^2$ + 0.01 * $X^7$

  (d) Repeat (c), using forward stepwise selection and also using backward stepwise selection. How does your answer compare to the results in (c)?
    
```{r}
#forward stepwise selection
leaps1 <- regsubsets(y~x+I(x^2)+I(x^3)+I(x^4)+I(x^5)+
                       I(x^6)+I(x^7)+I(x^8)+I(x^9)+I(x^10),
                     data=data.frame(x=x,y=y),nvmax=10, method="forward")
(summary1 <- summary(leaps1))
```
  
    
```{r}
#Cp
plot(1:10, summary1$cp, xlab="Number of Parameters", ylab="Cp", type = "l")
min_cp1 <- min(summary1$cp)
points(c(1:10)[summary1$cp==min_cp1], min_cp1, pch=1, col="purple")
```

```{r}
#BIC
plot(1:10, summary1$bic, xlab="Number of Parameters", ylab="BIC", type = "l")
min_bic1 <- min(summary1$bic)
points(c(1:10)[summary1$bic==min_bic1], min_bic1, pch=1, col="purple")
```

```{r}
#Adjusted R^2
plot(1:10, summary1$adjr2, xlab="Number of Parameters", ylab="Adjusted R^2", type = "l")
max_adjr2_1 <- max(summary1$adjr2)
points(c(1:10)[summary1$adjr2==max_adjr2_1], max_adjr2_1, pch=1, col="purple")
```


Using forward stepwise selection, the best model selected from $C_p$, BIC, and adjusted $R^2$ all contain 3 predictors which are $X, X^2, X^7$. The best model is the same as we got in (c) which is:
$Y$ = 3.08 + 2.36 * $X$-3.17* $X^2$ + 0.01 * $X^7$

Next let's do backwards stepwise selection:
```{r}
#backward
leapsback <- regsubsets(y~x+I(x^2)+I(x^3)+I(x^4)+I(x^5)+
                       I(x^6)+I(x^7)+I(x^8)+I(x^9)+I(x^10),  
                       data=data.frame(x=x,y=y),nvmax=10, method="backward")
summaryback <- summary(leapsback)
summaryback
```

```{r}
#Cp
plot(1:10, summaryback$cp, xlab="Number of Parameters", ylab="Cp", type = "l")
min_cpback <- min(summaryback$cp)
points(c(1:10)[summaryback$cp==min_cpback], min_cpback, pch=1, col="purple")
```


```{r}
#BIC
plot(1:10, summaryback$bic, xlab="Number of Parameters", ylab="BIC", type = "l")
min_bicback <- min(summaryback$bic)
points(c(1:10)[summaryback$bic==min_bicback], min_bicback, pch=1, col="purple")
```

```{r}
#Adjusted R^2
plot(1:10, summaryback$adjr2, xlab="Number of Parameters", ylab="Adjusted R^2", 
     type = "l")
max_adjr2_back <- max(summaryback$adjr2)
points(c(1:10)[summaryback$adjr2==max_adjr2_back], max_adjr2_back, pch=1, col="purple")
```

Using backwards stepwise selection, the best model selected from $C_p$, BIC, and adjusted $R^2$ all contain 3 predictors which are $X, X^2, X^9$. Next let's report the coefficients:

```{r}
#report the coefficients
lmback<-lm(y~I(x)+I(x^2)+I(x^9), data=data.frame(x=x,y=y) )
summary(lmback)
```

The best model according to backwards stepwise selection is:
$Y$ = 3.079 + 2.420 * $X$-3.177* $X^2$ + 0.002 * $X^9$
This model is different from that in (c).

  (e) Now fit a LASSO model with `glmnet` function from `glmnet` package to the simulated data, again using $(X,X^2,\cdots,X^{10})$ as predictors. Use cross-validation to select the optimal value of $\lambda$. Create plots of the cross-validation error as a function of $\lambda$. Report the resulting coefficient estimates, and discuss the results obtained.
    
```{r}
library(glmnet)
xmatrix= model.matrix(y ~ x + I(x^2) + I(x^3) + I(x^4) + 
                        I(x^5) + I(x^6) + I(x^7) + I(x^8) + I(x^9) + I(x^10),  
                      data = data.frame(x=x,y=y))[, -1]

# Cross-validation to select lambda
lasso.cv = cv.glmnet(xmatrix, y, alpha=1)
lasso.cv$lambda.min
plot(lasso.cv)

# coefficient estimate when refitting model with lamda.min
coef(lasso.cv, lasso.cv$lambda.min)
```

The optimal value of $\lambda$ is about 0.04. When we chose $\lambda$ which gives minimum cv error,
the Lasso method shrinks the coefficient of $X^3, X^4, X^6, X^8, X^9,X^{10}$to zero, and selects $X, X^2, X^5, X^7$ as predictors.


  (f) Now generate a response vector $Y$ according to the model $$Y = \beta_0 + \beta_7 X^7 + \epsilon,$$ where $\beta_7 = 7$, and perform best subset selection and the LASSO. Discuss the results obtained.

First let's perform best subset selection.
```{r}
y1 <- 3 + 7 * x^7 + epsilon
#best subsets
library(leaps)
sub_full <- regsubsets(y~x+I(x^2)+I(x^3)+I(x^4)+I(x^5)+I(x^6)+I(x^7)+I(x^8)
                    +I(x^9)+I(x^10),data=data.frame(x=x,y=y1),nvmax=10)
sub_summary <- summary(sub_full)
```



```{r}
#Cp
plot(1:10, sub_summary$cp, xlab="Number of Parameters", ylab="Cp", type = "l")
subset_min_cp <- min(sub_summary$cp)
points(c(1:10)[sub_summary$cp==subset_min_cp], subset_min_cp, pch=1, col="purple")
#report coefficient
coef(sub_full,2)
```

```{r}
#BIC
plot(1:10, sub_summary$bic, xlab="Number of Parameters", ylab="BIC", type = "l")
subset_min_bic <- min(sub_summary$bic)
points(c(1:10)[sub_summary$bic==subset_min_bic], subset_min_bic, pch=1, col="purple")
#report coefficient
coef(sub_full,1)
```

```{r}
#adjusted r^2
plot(1:10, sub_summary$adjr2, xlab="Number of Parameters", ylab="Adjusted R^2", type = "l")
subset_max_adjr2 <- max(sub_summary$adjr2)
points(c(1:10)[sub_summary$adjr2==subset_max_adjr2], subset_max_adjr2, pch=1, col="purple")
#report coefficient
coef(sub_full,4)
```

If we perform best subset selection, the best model obtained according to $C_p$ has 2 parameters, which are $X^2$ and $X^7$; the best model obtained according to $BIC$ has only 1 parameter, which is $X^7$; the best model obtained according to adjusted $R^2$ has 4 parameters, which are $X$, $X^2$, $X^3$ and $X^7$.

Let's do Lasso:
```{r}
library(glmnet)
xmatrix= model.matrix(y ~ x + I(x^2) + I(x^3) + I(x^4) + 
                        I(x^5) + I(x^6) + I(x^7) + I(x^8) + I(x^9) + I(x^10), 
                      data = data.frame(x=x,y=y))[, -1]

# Cross-validation to select lambda
lasso.cv1 = cv.glmnet(xmatrix, y1, alpha=1)
lasso.cv1$lambda.min
plot(lasso.cv)

# coefficient estimate when refitting model with lamda.min
coef(lasso.cv1, lasso.cv1$lambda.min)
```
If we perform Lasso, the best model has 1 parameter1, which is $X^7$. The model is$Y$ = 3.82 + 6.80 * $X^7$.

    
2. (Prediction, [ISL] 6.9, *20 pt*) In this exercise, we will predict the number of applications received (`Apps`) using the other variables in the `College` data set from `ISLR` package.

  (a) Randomly split the data set into equal sized training set and test set (1:1).
```{r}
library(ISLR)
data(College)
#split data in to 50% traing set and 50% test set
s <- sample(nrow(College), floor(nrow(College)*0.5), replace = F)
training <- College[s,]
test <- College[-s,]
```
    
    
  (b) Fit a linear model using least squares on the training set, and report the test error obtained.
```{r}
fit <- lm(Apps ~.,  data = training)
pred <- predict(fit, test)
mean((pred - test$Apps)^2)
```
    The test error is 1365049.
    
  (c) Fit a ridge regression model on the training set, with $\lambda$ chosen by 5-fold cross-validation. Report the test error obtained.
```{r}
xtrain=model.matrix (Apps~.,training)
ytrain=training$Apps
xtest=model.matrix (Apps~.,test)
ytest=test$Appscv
#use 5-fold cv:
cv.out=cv.glmnet (xtrain,ytrain,alpha =0, nfolds = 5)
plot(cv.out)
(bestlamda=cv.out$lambda.min)
#modelfrom ridge regression
ridge_rg = glmnet(xtrain,ytrain,alpha=0)
#test error
#Fitting trainning model on test set
ridge_pred <- predict(ridge_rg, s=bestlamda, newx = xtest)
#Calculating test error
mean((ridge_pred-ytest)^2)
```
The chosen $\lambda$ is 327.8098 and the test error result from ridge regression model is 2079222.
    
  (d) Fit a LASSO model on the training set, with $\lambda$ chosen by 5-fold cross-validation. Report the test error obtained, along with the number of non-zero coefficient estimates.
  
```{r}
#use 5-fold cv:
cv.outlasso=cv.glmnet (xtrain,ytrain,alpha =1, nfolds = 5)
plot(cv.outlasso)
(bestlamlasso=cv.outlasso$lambda.min)
#modelfrom lasso regression
fit.lasso <- glmnet(xtrain,ytrain,alpha=1)
lasso_rg = glmnet(xtrain,ytrain,alpha=1,lambda=bestlamlasso)
#the coefficient
lasso_coef <- coef(lasso_rg)
#test error
#Fitting trainning model on test set
lasso_pred=predict(fit.lasso, s=bestlamlasso,newx = xtest)
#Calculating test error
mean((lasso_pred - ytest)^2)
#the number of non-zero coefficient estimates.
predict(fit.lasso,type="coefficients",s=bestlamlasso)
```
The chosen $\lambda$ is 1.749422 The test error is 1359773. There are 16 non-zero coefficient estimates (not including intercept).


  (e) Comment on the results obtained. How accurately can we predict the number of college applications received? Is there much difference among the test errors resulting from these three approaches?
  
  To compare accuracy, we need to calculate $R^2$ for each approach:
```{r}
avgapps <- mean(ytest)
(lm_r2 <- 1 - mean((pred - test$Apps)^2) / mean((avgapps - ytest)^2))
(ridge_r2 <- 1 - mean((ridge_pred-ytest)^2) / 
  mean((avgapps - ytest)^2))
(lasso_r2 <- 1- mean((lasso_pred - ytest)^2) /
  mean((avgapps - ytest)^2))
```
  
The $R^2$ of least square linear model and lasso model is about 92%, which are higher than that of ridge regression model, which is about 88%. So the least square model and lasso model can predict the number of college applications received more accurately.

The model generated from Lasso method has much smaller test error (1359773) than that from Ridge regression method which is 2079222 in this dataset. The test error from linear model using least squares is similiar to the test error resulting from Lasso model which is 1365049.